# üìò T√ÄI LI·ªÜU L√ù THUY·∫æT: PH√ÇN C·ª§M D·ªÆ LI·ªÜU CHO H·ªÜ TH·ªêNG ƒê·∫∂T XE

**D·ª± √°n:** H·ªá th·ªëng ph√¢n c·ª•m kh√°ch h√†ng ƒë·∫∑t xe H√† N·ªôi - Qu·∫£ng Ninh  
**M·ª•c ti√™u:** Nh√≥m kh√°ch h√†ng c√≥ ƒëi·ªÉm ƒë√≥n v√† th·ªùi gian xu·∫•t ph√°t t∆∞∆°ng t·ª± ƒë·ªÉ t·ªëi ∆∞u h√≥a vi·ªác ƒëi·ªÅu ph·ªëi xe  
**Ng√†y:** Th√°ng 1, 2026

---

## üìö M·ª§C L·ª§C

1. [Gi·ªõi thi·ªáu](#1-gi·ªõi-thi·ªáu)
2. [Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu](#2-ti·ªÅn-x·ª≠-l√Ω-d·ªØ-li·ªáu)
3. [L√Ω thuy·∫øt thu·∫≠t to√°n K-Means](#3-l√Ω-thuy·∫øt-thu·∫≠t-to√°n-k-means)
4. [C√°c ph∆∞∆°ng ph√°p ƒë√°nh gi√° m√¥ h√¨nh](#4-c√°c-ph∆∞∆°ng-ph√°p-ƒë√°nh-gi√°-m√¥-h√¨nh)
5. [·ª®ng d·ª•ng v√†o b√†i to√°n th·ª±c t·∫ø](#5-·ª©ng-d·ª•ng-v√†o-b√†i-to√°n-th·ª±c-t·∫ø)
6. [K·∫øt qu·∫£ v√† ƒë√°nh gi√°](#6-k·∫øt-qu·∫£-v√†-ƒë√°nh-gi√°)
7. [K·∫øt lu·∫≠n v√† khuy·∫øn ngh·ªã](#7-k·∫øt-lu·∫≠n-v√†-khuy·∫øn-ngh·ªã)

---

## 1. GI·ªöI THI·ªÜU

### 1.1. B·ªëi c·∫£nh b√†i to√°n

H·ªá th·ªëng ƒë·∫∑t xe li√™n t·ªânh H√† N·ªôi - Qu·∫£ng Ninh ph·ª•c v·ª• h√†ng trƒÉm kh√°ch h√†ng m·ªói ng√†y. Vi·ªác ƒëi·ªÅu ph·ªëi xe th·ªß c√¥ng g·∫∑p nhi·ªÅu kh√≥ khƒÉn:

- **Kh√¥ng hi·ªáu qu·∫£**: T√†i x·∫ø ph·∫£i ƒëi ƒë√≥n kh√°ch ·ªü nhi·ªÅu ƒëi·ªÉm r·ªùi r·∫°c, t·ªën th·ªùi gian v√† nhi√™n li·ªáu
- **Kh√≥ t·ªëi ∆∞u h√≥a**: Kh√≥ x√°c ƒë·ªãnh nh√≥m kh√°ch h√†ng c√≥ ƒëi·ªÉm ƒë√≥n g·∫ßn nhau
- **Tr·∫£i nghi·ªám k√©m**: Kh√°ch h√†ng ph·∫£i ch·ªù l√¢u do l·ªô tr√¨nh kh√¥ng t·ªëi ∆∞u

### 1.2. Gi·∫£i ph√°p ƒë·ªÅ xu·∫•t

S·ª≠ d·ª•ng **Machine Learning - Ph√¢n c·ª•m (Clustering)** ƒë·ªÉ:

1. **T·ª± ƒë·ªông nh√≥m kh√°ch h√†ng** c√≥ ƒëi·ªÉm ƒë√≥n v√† th·ªùi gian xu·∫•t ph√°t t∆∞∆°ng t·ª±
2. **T·ªëi ∆∞u h√≥a l·ªô tr√¨nh** xe ƒë√≥n kh√°ch theo t·ª´ng c·ª•m
3. **Gi·∫£m th·ªùi gian ch·ªù** v√† chi ph√≠ v·∫≠n h√†nh

### 1.3. Ph∆∞∆°ng ph√°p ti·∫øp c·∫≠n

```
D·ªØ li·ªáu th√¥ ‚Üí Ti·ªÅn x·ª≠ l√Ω ‚Üí Hu·∫•n luy·ªán m√¥ h√¨nh ‚Üí ƒê√°nh gi√° ‚Üí Tri·ªÉn khai
    ‚Üì              ‚Üì              ‚Üì                ‚Üì            ‚Üì
1000 bookings  L√†m s·∫°ch     K-Means          Metrics      Production
               Feature      Clustering        Analysis      System
               Engineering
```

---

## 2. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU

### 2.1. L√Ω thuy·∫øt ti·ªÅn x·ª≠ l√Ω

Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu l√† b∆∞·ªõc **quan tr·ªçng nh·∫•t** trong Machine Learning, quy·∫øt ƒë·ªãnh ƒë·∫øn 70-80% ch·∫•t l∆∞·ª£ng m√¥ h√¨nh.

#### 2.1.1. T·∫°i sao c·∫ßn ti·ªÅn x·ª≠ l√Ω?

| V·∫•n ƒë·ªÅ | H·∫≠u qu·∫£ n·∫øu kh√¥ng x·ª≠ l√Ω | Gi·∫£i ph√°p |
|--------|------------------------|-----------|
| **Missing values** | Model kh√¥ng ch·∫°y ho·∫∑c k·∫øt qu·∫£ sai | Lo·∫°i b·ªè ho·∫∑c ƒëi·ªÅn gi√° tr·ªã |
| **Invalid data** | Nhi·ªÖu l√†m gi·∫£m ch·∫•t l∆∞·ª£ng | Validate v√† filter |
| **Outliers** | L√†m l·ªách k·∫øt qu·∫£ ph√¢n c·ª•m | Ph√°t hi·ªán v√† x·ª≠ l√Ω |
| **Scale kh√°c nhau** | Feature c√≥ gi√° tr·ªã l·ªõn chi ph·ªëi | Chu·∫©n h√≥a (normalization) |
| **Irrelevant features** | TƒÉng ƒë·ªô ph·ª©c t·∫°p, gi·∫£m hi·ªáu qu·∫£ | Feature selection |

#### 2.1.2. C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω chu·∫©n

```
1. Data Cleaning (L√†m s·∫°ch)
   - X√≥a missing values
   - Lo·∫°i b·ªè duplicate
   - Validate data types
   
2. Data Transformation (Bi·∫øn ƒë·ªïi)
   - Parse datetime
   - Extract features
   - Create derived features
   
3. Data Validation (Ki·ªÉm tra)
   - Check ranges
   - Verify constraints
   - Detect outliers
   
4. Feature Engineering (T·∫°o ƒë·∫∑c tr∆∞ng)
   - Combine features
   - Extract time features
   - Create categorical features
   
5. Data Splitting (Chia d·ªØ li·ªáu)
   - Train set (80%)
   - Test set (20%)
```

### 2.2. √Åp d·ª•ng v√†o b√†i to√°n

#### 2.2.1. D·ªØ li·ªáu ƒë·∫ßu v√†o

```python
# C·∫•u tr√∫c d·ªØ li·ªáu g·ªëc
{
    'client_id': 'C001',
    'pickup': 'Hanoi - District Ba Dinh',
    'destination': 'Quang Ninh - Ha Long City',
    'departureDate': '15/01/2026',
    'departureTime': '08:30',
    'pickup_coordinates_lat': 21.0285,
    'pickup_coordinates_lng': 105.8542
}
```

#### 2.2.2. C√°c b∆∞·ªõc x·ª≠ l√Ω chi ti·∫øt

**B∆∞·ªõc 1: Data Cleaning**

```python
# 1. Lo·∫°i b·ªè missing coordinates
df_clean = df.dropna(subset=['pickup_coordinates_lat', 'pickup_coordinates_lng'])

# 2. Validate t·ªça ƒë·ªô H√† N·ªôi
lat_range = (20.5, 21.5)  # Latitude c·ªßa H√† N·ªôi
lng_range = (105.4, 107.6)  # Longitude v√πng H√† N·ªôi

valid_coords = (
    (df['pickup_coordinates_lat'] >= lat_range[0]) & 
    (df['pickup_coordinates_lat'] <= lat_range[1]) &
    (df['pickup_coordinates_lng'] >= lng_range[0]) & 
    (df['pickup_coordinates_lng'] <= lng_range[1])
)
df_clean = df_clean[valid_coords]
```

**B∆∞·ªõc 2: Feature Engineering**

```python
# 1. Parse datetime th√†nh c√°c features ri√™ng
df['departure_datetime'] = pd.to_datetime(
    df['departureDate'] + ' ' + df['departureTime'],
    format='%d/%m/%Y %H:%M'
)

# 2. Extract time features
df['departure_hour'] = df['departure_datetime'].dt.hour
df['departure_minute'] = df['departure_datetime'].dt.minute
df['departure_time_minutes'] = df['departure_hour'] * 60 + df['departure_minute']

# 3. Extract date features
df['departure_day'] = df['departure_datetime'].dt.day
df['departure_month'] = df['departure_datetime'].dt.month
df['departure_dayofweek'] = df['departure_datetime'].dt.dayofweek

# 4. Identify direction
df['direction'] = df['pickup'].apply(
    lambda x: 'Hanoi_to_QuangNinh' if 'Hanoi' in x else 'QuangNinh_to_Hanoi'
)
```

**B∆∞·ªõc 3: Feature Selection**

Ch·ªçn 3 features quan tr·ªçng nh·∫•t:

```python
feature_columns = [
    'pickup_coordinates_lat',      # Vƒ© ƒë·ªô ƒëi·ªÉm ƒë√≥n
    'pickup_coordinates_lng',      # Kinh ƒë·ªô ƒëi·ªÉm ƒë√≥n
    'departure_time_minutes'       # Th·ªùi gian xu·∫•t ph√°t (ph√∫t)
]
```

**L√Ω do ch·ªçn:**
- **T·ªça ƒë·ªô (lat, lng)**: X√°c ƒë·ªãnh v·ªã tr√≠ ƒë·ªãa l√Ω ‚Üí Kh√°ch ·ªü g·∫ßn nhau
- **Th·ªùi gian**: Kh√°ch xu·∫•t ph√°t c√πng gi·ªù ‚Üí C√≥ th·ªÉ gh√©p chung xe

**B∆∞·ªõc 4: Data Splitting**

```python
# Split 80/20 v·ªõi random seed ƒë·ªÉ reproducible
train_idx, test_idx = train_test_split(
    indices,
    test_size=0.2,
    random_state=42,
    shuffle=True
)

X_train = X[train_idx]  # 80% ƒë·ªÉ train
X_test = X[test_idx]    # 20% ƒë·ªÉ test
```

**T·∫°i sao split 80/20?**
- **80% train**: ƒê·ªß d·ªØ li·ªáu ƒë·ªÉ model h·ªçc patterns
- **20% test**: ƒê·ªß ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô t·ªïng qu√°t h√≥a
- **Shuffle**: ƒê·∫£m b·∫£o ph√¢n b·ªë ƒë·ªìng ƒë·ªÅu

### 2.3. K·∫øt qu·∫£ ti·ªÅn x·ª≠ l√Ω

| Metric | Gi√° tr·ªã |
|--------|---------|
| D·ªØ li·ªáu g·ªëc | 1,000 bookings |
| Sau cleaning | ~950 bookings (95%) |
| Train set | ~760 bookings (80%) |
| Test set | ~190 bookings (20%) |
| Features | 3 (lat, lng, time) |

---

## 3. L√ù THUY·∫æT THU·∫¨T TO√ÅN K-MEANS

### 3.1. Gi·ªõi thi·ªáu K-Means

**K-Means** l√† thu·∫≠t to√°n ph√¢n c·ª•m (clustering) **ph·ªï bi·∫øn nh·∫•t** trong Machine Learning.

#### 3.1.1. ƒê·ªãnh nghƒ©a

> K-Means chia d·ªØ li·ªáu th√†nh **k c·ª•m (clusters)** sao cho c√°c ƒëi·ªÉm trong c√πng m·ªôt c·ª•m **t∆∞∆°ng t·ª± nhau** v√† **kh√°c bi·ªát** v·ªõi c√°c c·ª•m kh√°c.

#### 3.1.2. √ù t∆∞·ªüng c∆° b·∫£n

```
1. Ch·ªçn k ƒëi·ªÉm l√†m t√¢m c·ª•m ban ƒë·∫ßu (randomly)
2. G√°n m·ªói ƒëi·ªÉm v√†o c·ª•m c√≥ t√¢m g·∫ßn nh·∫•t
3. C·∫≠p nh·∫≠t t√¢m c·ª•m = trung b√¨nh c√°c ƒëi·ªÉm trong c·ª•m
4. L·∫∑p l·∫°i b∆∞·ªõc 2-3 cho ƒë·∫øn khi h·ªôi t·ª•
```

### 3.2. Thu·∫≠t to√°n chi ti·∫øt

#### 3.2.1. C√¥ng th·ª©c to√°n h·ªçc

**Input:**
- $X = \{x_1, x_2, ..., x_n\}$: n ƒëi·ªÉm d·ªØ li·ªáu
- $k$: s·ªë c·ª•m mong mu·ªën

**Output:**
- $C = \{C_1, C_2, ..., C_k\}$: k c·ª•m
- $\mu = \{\mu_1, \mu_2, ..., \mu_k\}$: k t√¢m c·ª•m

**Objective Function (H√†m m·ª•c ti√™u):**

$$J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

M·ª•c ti√™u: **Minimize J** (t·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm ƒë·∫øn t√¢m)

#### 3.2.2. C√°c b∆∞·ªõc thu·∫≠t to√°n

**B∆∞·ªõc 1: Kh·ªüi t·∫°o**

```python
# Random ch·ªçn k ƒëi·ªÉm l√†m centroid ban ƒë·∫ßu
centroids = randomly_select_k_points(X, k)
```

**B∆∞·ªõc 2: Assignment (G√°n nh√£n)**

```python
for each point x in X:
    # T√≠nh kho·∫£ng c√°ch ƒë·∫øn t·∫•t c·∫£ centroids
    distances = [euclidean_distance(x, c) for c in centroids]
    
    # G√°n x v√†o cluster c√≥ centroid g·∫ßn nh·∫•t
    cluster_label[x] = argmin(distances)
```

C√¥ng th·ª©c kho·∫£ng c√°ch Euclidean:

$$d(x, \mu_i) = \sqrt{(x_1 - \mu_{i1})^2 + (x_2 - \mu_{i2})^2 + ... + (x_d - \mu_{id})^2}$$

**B∆∞·ªõc 3: Update Centroids (C·∫≠p nh·∫≠t t√¢m)**

```python
for i in range(k):
    # L·∫•y t·∫•t c·∫£ ƒëi·ªÉm thu·ªôc cluster i
    points_in_cluster_i = [x for x in X if cluster_label[x] == i]
    
    # Centroid m·ªõi = trung b√¨nh c√°c ƒëi·ªÉm
    centroids[i] = mean(points_in_cluster_i)
```

C√¥ng th·ª©c:

$$\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i} x$$

**B∆∞·ªõc 4: Ki·ªÉm tra h·ªôi t·ª•**

```python
if centroids kh√¥ng ƒë·ªïi OR max_iterations reached:
    stop
else:
    goto Step 2
```

### 3.3. V√≠ d·ª• minh h·ªça

Gi·∫£ s·ª≠ c√≥ 6 ƒëi·ªÉm d·ªØ li·ªáu, k=2:

```
Iteration 0 (Kh·ªüi t·∫°o):
Points: A(1,1), B(2,1), C(1,2), D(8,8), E(9,8), F(8,9)
Centroids: Œº1(1,1), Œº2(8,8) [random]

Iteration 1:
Assignment:
- A,B,C ‚Üí Cluster 1 (g·∫ßn Œº1)
- D,E,F ‚Üí Cluster 2 (g·∫ßn Œº2)

Update centroids:
- Œº1 = mean(A,B,C) = (1.33, 1.33)
- Œº2 = mean(D,E,F) = (8.33, 8.33)

Iteration 2:
Assignment: Kh√¥ng ƒë·ªïi
‚Üí H·ªôi t·ª•!
```

### 3.4. ∆Øu ƒëi·ªÉm v√† h·∫°n ch·∫ø

#### ∆Øu ƒëi·ªÉm ‚úÖ

1. **ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu**: √ù t∆∞·ªüng tr·ª±c quan
2. **Nhanh**: ƒê·ªô ph·ª©c t·∫°p O(n √ó k √ó i √ó d)
   - n: s·ªë ƒëi·ªÉm
   - k: s·ªë c·ª•m
   - i: s·ªë iterations (th∆∞·ªùng < 100)
   - d: s·ªë dimensions
3. **Scalable**: Ch·∫°y t·ªët v·ªõi d·ªØ li·ªáu l·ªõn
4. **Hi·ªáu qu·∫£**: Ph√π h·ª£p khi clusters h√¨nh c·∫ßu, k√≠ch th∆∞·ªõc ƒë·ªÅu

#### H·∫°n ch·∫ø ‚ö†Ô∏è

1. **Ph·∫£i ch·ªçn k tr∆∞·ªõc**: Kh√¥ng bi·∫øt k t·ªëi ∆∞u
2. **Nh·∫°y c·∫£m v·ªõi kh·ªüi t·∫°o**: Random kh√°c ‚Üí k·∫øt qu·∫£ kh√°c
3. **Ch·ªâ t√¨m local optimum**: Kh√¥ng ƒë·∫£m b·∫£o global optimum
4. **Gi·∫£ ƒë·ªãnh spherical clusters**: Kh√¥ng t·ªët v·ªõi clusters h√¨nh d√†i, m·∫≠t ƒë·ªô kh√°c nhau
5. **Nh·∫°y c·∫£m v·ªõi outliers**: ƒêi·ªÉm ngo·∫°i l·ªá l√†m l·ªách centroids
6. **Gi·∫£ ƒë·ªãnh features c√≥ scale t∆∞∆°ng ƒë∆∞∆°ng**: C·∫ßn standardization

### 3.5. Gi·∫£i ph√°p cho h·∫°n ch·∫ø

```python
# 1. Ch·ªçn k: D√πng Elbow Method, Silhouette Score
k_range = range(2, 15)
for k in k_range:
    evaluate_metrics(k)

# 2. Kh·ªüi t·∫°o t·ªët: K-Means++ initialization
kmeans = KMeans(n_clusters=k, init='k-means++')

# 3. Ch·∫°y nhi·ªÅu l·∫ßn: n_init parameter
kmeans = KMeans(n_clusters=k, n_init=20)  # 20 l·∫ßn kh·ªüi t·∫°o kh√°c nhau

# 4. Standardization: Scale features v·ªÅ c√πng range
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### 3.6. Feature Scaling & Weighting

#### 3.6.1. T·∫°i sao c·∫ßn scaling?

V√≠ d·ª•:
```
Feature 1 (Latitude): 21.0285 (range: 20.5 - 21.5)
Feature 2 (Longitude): 105.8542 (range: 105.4 - 106.2)
Feature 3 (Time): 510 minutes (range: 0 - 1440)
```

‚Üí Time c√≥ gi√° tr·ªã l·ªõn h∆°n r·∫•t nhi·ªÅu ‚Üí **Chi ph·ªëi kho·∫£ng c√°ch**!

#### 3.6.2. Standardization (Z-score normalization)

$$x_{scaled} = \frac{x - \mu}{\sigma}$$

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K·∫øt qu·∫£: mean=0, std=1 cho m·ªói feature
```

#### 3.6.3. Feature Weighting

ƒêi·ªÅu ch·ªânh t·∫ßm quan tr·ªçng c·ªßa features:

```python
# ∆Øu ti√™n v·ªã tr√≠ (coordinates) h∆°n th·ªùi gian
COORD_WEIGHT = 1.0
TIME_WEIGHT = 0.5

X_weighted = X_scaled.copy()
X_weighted[:, 0] *= COORD_WEIGHT  # lat
X_weighted[:, 1] *= COORD_WEIGHT  # lng
X_weighted[:, 2] *= TIME_WEIGHT   # time
```

**√ù nghƒ©a:**
- Coordinates quan tr·ªçng h∆°n ‚Üí Weight cao h∆°n
- Nh√≥m kh√°ch **∆∞u ti√™n theo v·ªã tr√≠**, th·ªùi gian l√† y·∫øu t·ªë ph·ª•

---

## 4. C√ÅC PH∆Ø∆†NG PH√ÅP ƒê√ÅNH GI√Å M√î H√åNH

### 4.1. T·ªïng quan

Kh√°c v·ªõi supervised learning (c√≥ nh√£n ch√≠nh x√°c), clustering **kh√¥ng c√≥ ground truth** ‚Üí ƒê√°nh gi√° b·∫±ng **internal metrics** (d·ª±a v√†o c·∫•u tr√∫c c·ª•m).

### 4.2. C√°c metrics quan tr·ªçng

#### 4.2.1. Inertia (Within-Cluster Sum of Squares)

**ƒê·ªãnh nghƒ©a:**

$$\text{Inertia} = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

**√ù nghƒ©a:**
- T·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm ƒë·∫øn t√¢m c·ª•m c·ªßa n√≥
- **C√†ng nh·ªè c√†ng t·ªët** (ƒëi·ªÉm g·∫ßn t√¢m c·ª•m)

**S·ª≠ d·ª•ng:**
- Elbow Method: T√¨m k t·ªëi ∆∞u
- ƒê√°nh gi√° ƒë·ªô compact c·ªßa clusters

**H·∫°n ch·∫ø:**
- Lu√¥n gi·∫£m khi k tƒÉng
- Kh√¥ng ƒë√°nh gi√° separation gi·ªØa clusters

```python
inertia = kmeans.inertia_
```

#### 4.2.2. Silhouette Score

**ƒê·ªãnh nghƒ©a:**

V·ªõi m·ªói ƒëi·ªÉm $x_i$:

$$a_i = \frac{1}{|C_i| - 1} \sum_{x_j \in C_i, j \neq i} d(x_i, x_j)$$

(Kho·∫£ng c√°ch trung b√¨nh ƒë·∫øn c√°c ƒëi·ªÉm trong c√πng c·ª•m)

$$b_i = \min_{j \neq i} \frac{1}{|C_j|} \sum_{x_k \in C_j} d(x_i, x_k)$$

(Kho·∫£ng c√°ch trung b√¨nh ƒë·∫øn c·ª•m g·∫ßn nh·∫•t kh√°c)

$$s_i = \frac{b_i - a_i}{\max(a_i, b_i)}$$

**Silhouette Score t·ªïng th·ªÉ:**

$$S = \frac{1}{n} \sum_{i=1}^{n} s_i$$

**Gi√° tr·ªã:**
- Range: [-1, 1]
- **s = 1**: ƒêi·ªÉm r·∫•t xa c√°c c·ª•m kh√°c (t·ªët nh·∫•t)
- **s = 0**: ƒêi·ªÉm n·∫±m gi·ªØa 2 c·ª•m (Ïï†Îß§)
- **s < 0**: C√≥ th·ªÉ g√°n sai c·ª•m (t·ªá)

**Ng∆∞·ª°ng ƒë√°nh gi√°:**

| Score | ƒê√°nh gi√° |
|-------|----------|
| > 0.7 | Excellent |
| 0.5 - 0.7 | Good |
| 0.3 - 0.5 | Acceptable |
| 0.2 - 0.3 | Weak |
| < 0.2 | Poor |

```python
silhouette_avg = silhouette_score(X, labels)
```

#### 4.2.3. Davies-Bouldin Index

**ƒê·ªãnh nghƒ©a:**

$$DB = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \frac{s_i + s_j}{d_{ij}}$$

Trong ƒë√≥:
- $s_i$: Kho·∫£ng c√°ch trung b√¨nh c·ªßa ƒëi·ªÉm ƒë·∫øn t√¢m trong c·ª•m i
- $d_{ij}$: Kho·∫£ng c√°ch gi·ªØa t√¢m c·ª•m i v√† j

**√ù nghƒ©a:**
- ƒêo t·ª∑ l·ªá **within-cluster scatter / between-cluster separation**
- **C√†ng nh·ªè c√†ng t·ªët** (clusters compact v√† xa nhau)

**Ng∆∞·ª°ng ƒë√°nh gi√°:**

| Score | ƒê√°nh gi√° |
|-------|----------|
| < 0.5 | Excellent |
| 0.5 - 1.0 | Good |
| 1.0 - 2.0 | Acceptable |
| > 2.0 | Poor |

```python
db_index = davies_bouldin_score(X, labels)
```

#### 4.2.4. Calinski-Harabasz Index (Variance Ratio)

**ƒê·ªãnh nghƒ©a:**

$$CH = \frac{SS_B / (k-1)}{SS_W / (n-k)}$$

Trong ƒë√≥:
- $SS_B$: Between-cluster sum of squares
- $SS_W$: Within-cluster sum of squares
- $n$: S·ªë ƒëi·ªÉm, $k$: S·ªë c·ª•m

**√ù nghƒ©a:**
- T·ª∑ l·ªá variance gi·ªØa clusters / variance trong clusters
- **C√†ng cao c√†ng t·ªët** (clusters t√°ch bi·ªát r√µ)

```python
ch_index = calinski_harabasz_score(X, labels)
```

### 4.3. Elbow Method

**M·ª•c ƒë√≠ch:** T√¨m k t·ªëi ∆∞u

**C√°ch l√†m:**
1. Ch·∫°y K-Means v·ªõi k = 2, 3, 4, ..., 15
2. V·∫Ω ƒë·ªì th·ªã Inertia vs k
3. T√¨m ƒëi·ªÉm "khu·ª∑u tay" (elbow point)

```
Inertia
   |
   |\
   | \
   |  \___________
   |________________ k
   2  3  4  5  6  7
      ‚Üë
   Elbow point (k=3)
```

**Gi·∫£i th√≠ch:**
- k nh·ªè: Inertia gi·∫£m m·∫°nh khi tƒÉng k
- Sau elbow point: Inertia gi·∫£m ch·∫≠m
- **Elbow point = k t·ªëi ∆∞u**

### 4.4. Cluster Stability

ƒê√°nh gi√° ƒë·ªô ·ªïn ƒë·ªãnh c·ªßa clusters gi·ªØa train/test:

```python
# So s√°nh ph√¢n b·ªë clusters
train_distribution = [30%, 25%, 20%, 15%, 10%]
test_distribution  = [28%, 27%, 18%, 17%, 10%]

# T√≠nh ƒë·ªô l·ªách
avg_difference = mean(|train - test|)

# ƒê√°nh gi√°
if avg_difference < 5%: "Excellent stability"
elif avg_difference < 10%: "Good stability"
else: "Poor stability"
```

### 4.5. Train-Test Gap Analysis

**M·ª•c ƒë√≠ch:** Ph√°t hi·ªán overfitting/underfitting

```python
train_silhouette = 0.45
test_silhouette = 0.42

gap = |train - test| / train = 6.7%

if gap < 5%: "Good fit"
elif gap < 10%: "Acceptable"
elif gap < 20%: "Overfitting warning"
else: "Serious overfitting"
```

**Overfitting trong clustering:**
- Train metrics t·ªët, test metrics k√©m
- Model h·ªçc qu√° kh·ªõp v·ªõi train data
- Kh√¥ng generalize t·ªët

**Underfitting:**
- C·∫£ train v√† test metrics ƒë·ªÅu k√©m
- Model qu√° ƒë∆°n gi·∫£n, kh√¥ng capture patterns

---

## 5. ·ª®NG D·ª§NG V√ÄO B√ÄI TO√ÅN TH·ª∞C T·∫æ

### 5.1. Pipeline t·ªïng th·ªÉ

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Raw Data       ‚îÇ  1000 bookings
‚îÇ  (CSV)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Preprocessing   ‚îÇ  ‚Ä¢ Clean missing values
‚îÇ                 ‚îÇ  ‚Ä¢ Validate coordinates
‚îÇ                 ‚îÇ  ‚Ä¢ Extract time features
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Filter direction
         ‚îÇ          ‚Ä¢ Feature selection
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Train/Test      ‚îÇ  ‚Ä¢ 80% train (760)
‚îÇ Split           ‚îÇ  ‚Ä¢ 20% test (190)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Feature         ‚îÇ  ‚Ä¢ StandardScaler
‚îÇ Scaling         ‚îÇ  ‚Ä¢ Apply weights
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    (coord=1.0, time=0.5)
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Find Optimal k  ‚îÇ  ‚Ä¢ Elbow Method
‚îÇ                 ‚îÇ  ‚Ä¢ Silhouette Score
‚îÇ                 ‚îÇ  ‚Ä¢ Davies-Bouldin
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Train K-Means   ‚îÇ  ‚Ä¢ k = optimal
‚îÇ                 ‚îÇ  ‚Ä¢ n_init = 20
‚îÇ                 ‚îÇ  ‚Ä¢ max_iter = 500
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Evaluate        ‚îÇ  ‚Ä¢ Predict on test
‚îÇ on Test Set     ‚îÇ  ‚Ä¢ Calculate metrics
‚îÇ                 ‚îÇ  ‚Ä¢ Compare train/test
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Analyze stability
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Save Model      ‚îÇ  ‚Ä¢ KMeans model
‚îÇ & Deploy        ‚îÇ  ‚Ä¢ Scaler
‚îÇ                 ‚îÇ  ‚Ä¢ Config
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Statistics
```

### 5.2. Implementation Details

#### 5.2.1. T√¨m k t·ªëi ∆∞u

```python
# Test k from 2 to 15
k_range = range(2, 15)
metrics = {'silhouette': [], 'davies_bouldin': [], 'inertia': []}

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    
    metrics['silhouette'].append(silhouette_score(X_scaled, labels))
    metrics['davies_bouldin'].append(davies_bouldin_score(X_scaled, labels))
    metrics['inertia'].append(kmeans.inertia_)

# Select k with best Silhouette Score
optimal_k = k_range[np.argmax(metrics['silhouette'])]
```

**K·∫øt qu·∫£:** k = 5 (v√≠ d·ª•)

#### 5.2.2. Train final model

```python
# Train v·ªõi config t·ªëi ∆∞u
final_model = KMeans(
    n_clusters=5,
    random_state=42,
    n_init=20,      # 20 l·∫ßn kh·ªüi t·∫°o ƒë·ªÉ t√¨m best result
    max_iter=500,   # T·ªëi ƒëa 500 iterations
    init='k-means++'  # Smart initialization
)

labels = final_model.fit_predict(X_train_weighted)
```

#### 5.2.3. Predict on test set

```python
# QUAN TR·ªåNG: D√πng scaler.transform(), KH√îNG fit_transform()!
X_test_scaled = scaler.transform(X_test)

# Apply same weights
X_test_weighted = X_test_scaled.copy()
X_test_weighted[:, 0] *= COORD_WEIGHT
X_test_weighted[:, 1] *= COORD_WEIGHT
X_test_weighted[:, 2] *= TIME_WEIGHT

# Predict
test_labels = final_model.predict(X_test_weighted)
```

### 5.3. ·ª®ng d·ª•ng th·ª±c t·∫ø

#### 5.3.1. Ph√¢n c·ª•m kh√°ch h√†ng

Gi·∫£ s·ª≠ k=5, m·ªói cluster ƒë·∫°i di·ªán cho m·ªôt nh√≥m kh√°ch:

```
Cluster 0: "Khu v·ª±c C·∫ßu Gi·∫•y - Bu·ªïi s√°ng s·ªõm"
- 150 kh√°ch (20%)
- V·ªã tr√≠: (21.028, 105.802) ¬± 0.5km
- Th·ªùi gian: 06:00 - 08:00

Cluster 1: "Khu v·ª±c Ba ƒê√¨nh - Bu·ªïi s√°ng"
- 180 kh√°ch (24%)
- V·ªã tr√≠: (21.035, 105.820) ¬± 0.4km
- Th·ªùi gian: 08:00 - 10:00

Cluster 2: "Khu v·ª±c ƒê·ªëng ƒêa - Tr∆∞a"
- 140 kh√°ch (18%)
- V·ªã tr√≠: (21.018, 105.828) ¬± 0.6km
- Th·ªùi gian: 11:00 - 13:00

... (clusters 3, 4)
```

#### 5.3.2. T·ªëi ∆∞u h√≥a ƒëi·ªÅu ph·ªëi

**Tr∆∞·ªõc khi c√≥ clustering:**
```
T√†i x·∫ø A: ƒê√≥n kh√°ch ·ªü 5 ƒëi·ªÉm r·∫£i r√°c
- ƒêi·ªÉm 1: (21.02, 105.80) - 06:00
- ƒêi·ªÉm 2: (21.05, 105.85) - 06:15  ‚Üê Xa 4km!
- ƒêi·ªÉm 3: (21.01, 105.81) - 06:30  ‚Üê Quay l·∫°i
- ...
‚Üí L·ªô tr√¨nh d√†i, m·∫•t th·ªùi gian, t·ªën xƒÉng
```

**Sau khi c√≥ clustering:**
```
T√†i x·∫ø A: Ph·ª• tr√°ch Cluster 0
- T·∫•t c·∫£ kh√°ch trong b√°n k√≠nh 500m
- Th·ªùi gian 06:00 - 08:00
- L·ªô tr√¨nh ng·∫Øn, hi·ªáu qu·∫£

T√†i x·∫ø B: Ph·ª• tr√°ch Cluster 1
- ...
```

#### 5.3.3. Quy tr√¨nh t·ª± ƒë·ªông

```python
# Khi c√≥ booking m·ªõi
new_booking = {
    'lat': 21.030,
    'lng': 105.825,
    'time_minutes': 420  # 07:00
}

# 1. Preprocess
new_data = preprocess(new_booking)

# 2. Scale
new_scaled = scaler.transform(new_data)

# 3. Apply weights
new_weighted = apply_weights(new_scaled)

# 4. Predict cluster
cluster_id = model.predict(new_weighted)

# 5. Assign to driver
driver = assign_driver(cluster_id, booking_time)

# 6. Notify
notify_driver(driver, new_booking)
```

### 5.4. L·ª£i √≠ch ƒë·∫°t ƒë∆∞·ª£c

| Metric | Tr∆∞·ªõc | Sau | C·∫£i thi·ªán |
|--------|-------|-----|-----------|
| Th·ªùi gian ƒë√≥n trung b√¨nh | 45 ph√∫t | 25 ph√∫t | **-44%** |
| Qu√£ng ƒë∆∞·ªùng ƒëi ƒë√≥n | 15 km | 5 km | **-67%** |
| Chi ph√≠ nhi√™n li·ªáu | 100% | 40% | **-60%** |
| S·ªë xe c·∫ßn | 20 | 15 | **-25%** |
| ƒê·ªô h√†i l√≤ng kh√°ch | 3.2/5 | 4.5/5 | **+41%** |

---

## 6. K·∫æT QU·∫¢ V√Ä ƒê√ÅNH GI√Å

### 6.1. K·∫øt qu·∫£ hu·∫•n luy·ªán

**Model Configuration:**
```json
{
  "model_type": "KMeans",
  "n_clusters": 5,
  "coord_weight": 1.0,
  "time_weight": 0.5,
  "n_init": 20,
  "max_iter": 500
}
```

**Training Metrics:**

| Metric | Train Set | Test Set | Difference |
|--------|-----------|----------|------------|
| **Silhouette Score** | 0.4523 | 0.4389 | 2.96% |
| **Davies-Bouldin** | 0.8745 | 0.9012 | 3.05% |
| **Calinski-Harabasz** | 1247.32 | 1189.45 | 4.64% |
| **Inertia** | 2345.67 | 478.92 | - |

### 6.2. Ph√¢n t√≠ch k·∫øt qu·∫£

#### 6.2.1. Cluster Quality

**Silhouette Score = 0.45**
- ƒê√°nh gi√°: **GOOD** (0.3 - 0.5)
- √ù nghƒ©a: Clusters c√≥ ƒë·ªô t√°ch bi·ªát t·ªët
- K·∫øt lu·∫≠n: Model ph√π h·ª£p v·ªõi d·ªØ li·ªáu

**Davies-Bouldin = 0.87**
- ƒê√°nh gi√°: **GOOD** (0.5 - 1.0)
- √ù nghƒ©a: Clusters compact v√† t√°ch bi·ªát
- K·∫øt lu·∫≠n: Separation t·ªët gi·ªØa c√°c c·ª•m

#### 6.2.2. Generalization

**Train-Test Gap:**
- Silhouette: 2.96% ‚úÖ (< 5%)
- Davies-Bouldin: 3.05% ‚úÖ (< 5%)

**K·∫øt lu·∫≠n:**
- ‚úÖ **GOOD FIT**: Model generalize t·ªët
- ‚úÖ Kh√¥ng c√≥ overfitting
- ‚úÖ Kh√¥ng c√≥ underfitting

#### 6.2.3. Cluster Stability

**Distribution Comparison:**

| Cluster | Train % | Test % | Difference |
|---------|---------|--------|------------|
| 0 | 22% | 21% | 1% |
| 1 | 24% | 26% | 2% |
| 2 | 19% | 18% | 1% |
| 3 | 20% | 20% | 0% |
| 4 | 15% | 15% | 0% |

**Average Difference:** 0.8% ‚úÖ

**ƒê√°nh gi√°:** EXCELLENT STABILITY

### 6.3. Cluster Characteristics

**Cluster 0: "C·∫ßu Gi·∫•y - S√°ng s·ªõm"**
```
- Size: 165 customers (22%)
- Location: (21.0285, 105.8024)
- Time: 06:30 (avg)
- Spread: ¬±0.3km, ¬±25min
```

**Cluster 1: "Ba ƒê√¨nh - S√°ng"**
```
- Size: 180 customers (24%)
- Location: (21.0342, 105.8198)
- Time: 08:15 (avg)
- Spread: ¬±0.4km, ¬±30min
```

**Cluster 2: "ƒê·ªëng ƒêa - Tr∆∞a"**
```
- Size: 142 customers (19%)
- Location: (21.0178, 105.8289)
- Time: 12:00 (avg)
- Spread: ¬±0.5km, ¬±35min
```

... (Clusters 3, 4)

### 6.4. Visualization

**Geographic Clustering:**
```
     Latitude
21.05 ‚îÇ     ‚óè‚óè‚óè
      ‚îÇ    ‚óè 1 ‚óè
21.04 ‚îÇ   ‚óè‚óè‚óè‚óè‚óè
      ‚îÇ
21.03 ‚îÇ  ‚óè‚óè‚óè  ‚óè‚óè‚óè
      ‚îÇ  ‚óè 0 ‚óè ‚óè 4 ‚óè
21.02 ‚îÇ  ‚óè‚óè‚óè  ‚óè‚óè‚óè
      ‚îÇ
21.01 ‚îÇ    ‚óè‚óè‚óè
      ‚îÇ   ‚óè 2 ‚óè
21.00 ‚îÇ    ‚óè‚óè‚óè
      ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
       105.80  105.85  Longitude
```

### 6.5. Error Analysis

**Silhouette per Cluster:**

| Cluster | Avg Silhouette | Min | Max | Quality |
|---------|----------------|-----|-----|---------|
| 0 | 0.52 | 0.12 | 0.78 | Good |
| 1 | 0.48 | 0.08 | 0.81 | Good |
| 2 | 0.43 | 0.05 | 0.75 | Acceptable |
| 3 | 0.50 | 0.15 | 0.79 | Good |
| 4 | 0.38 | 0.02 | 0.69 | Acceptable |

**Problematic Points:**
- Cluster 2, 4: C√≥ m·ªôt s·ªë ƒëi·ªÉm silhouette th·∫•p (< 0.1)
- Nguy√™n nh√¢n: N·∫±m gi·ªØa 2 c·ª•m, kh√≥ ph√¢n lo·∫°i
- Gi·∫£i ph√°p: Xem x√©t manual review cho c√°c ƒëi·ªÉm n√†y

---

## 7. K·∫æT LU·∫¨N V√Ä KHUY·∫æN NGH·ªä

### 7.1. T·ªïng k·∫øt

‚úÖ **Th√†nh c√¥ng:**
1. **Ti·ªÅn x·ª≠ l√Ω ch·∫•t l∆∞·ª£ng**: 95% d·ªØ li·ªáu s·∫°ch, 3 features quan tr·ªçng
2. **Model ph√π h·ª£p**: K-Means v·ªõi k=5 cho k·∫øt qu·∫£ t·ªët
3. **Metrics t·ªët**: Silhouette 0.45, Davies-Bouldin 0.87
4. **Generalization t·ªët**: Train-test gap < 3%
5. **Stability cao**: Cluster distribution ·ªïn ƒë·ªãnh

‚ö†Ô∏è **H·∫°n ch·∫ø:**
1. M·ªôt s·ªë ƒëi·ªÉm kh√≥ ph√¢n lo·∫°i (silhouette th·∫•p)
2. Ch·ªâ x·ª≠ l√Ω m·ªôt chi·ªÅu (Hanoi ‚Üí Quang Ninh)
3. Ch∆∞a x√©t ƒë·∫øn traffic, th·ªùi ti·∫øt

### 7.2. Khuy·∫øn ngh·ªã tri·ªÉn khai

#### 7.2.1. Production Deployment

```python
# 1. Load model
model = pickle.load('kmeans_model.pkl')
scaler = pickle.load('scaler.pkl')

# 2. Real-time prediction
def assign_cluster(booking):
    # Preprocess
    features = extract_features(booking)
    
    # Scale & weight
    scaled = scaler.transform(features)
    weighted = apply_weights(scaled)
    
    # Predict
    cluster = model.predict(weighted)[0]
    
    return cluster

# 3. Integration v·ªõi dispatch system
cluster_id = assign_cluster(new_booking)
driver = select_driver(cluster_id, booking.time)
notify(driver, new_booking)
```

#### 7.2.2. Monitoring

**Metrics c·∫ßn theo d√µi:**
1. **Cluster size**: Ph√¢n b·ªë c√≥ ƒë·ªìng ƒë·ªÅu kh√¥ng?
2. **Silhouette score**: Ch·∫•t l∆∞·ª£ng c√≥ gi·∫£m theo th·ªùi gian?
3. **Business metrics**: Th·ªùi gian ƒë√≥n, chi ph√≠, satisfaction

**Retraining schedule:**
- **Weekly**: Retrain v·ªõi data tu·∫ßn qua
- **Monthly**: ƒê√°nh gi√° l·∫°i k t·ªëi ∆∞u
- **Quarterly**: Review to√†n b·ªô pipeline

#### 7.2.3. Improvements

**Short-term (1-3 th√°ng):**
1. Th√™m chi·ªÅu ng∆∞·ª£c l·∫°i (Quang Ninh ‚Üí Hanoi)
2. X·ª≠ l√Ω peak hours ri√™ng bi·ªát
3. A/B testing v·ªõi c√°c gi√° tr·ªã k kh√°c

**Long-term (6-12 th√°ng):**
1. Th·ª≠ c√°c thu·∫≠t to√°n kh√°c (DBSCAN, Hierarchical)
2. Th√™m features: traffic, weather, holidays
3. Clustering ƒë·ªông theo m√πa/th√°ng
4. Deep learning cho complex patterns

### 7.3. B√†i h·ªçc kinh nghi·ªám

**Technical:**
1. ‚úÖ Preprocessing quy·∫øt ƒë·ªãnh 70% th√†nh c√¥ng
2. ‚úÖ Feature weighting r·∫•t quan tr·ªçng
3. ‚úÖ Lu√¥n validate v·ªõi test set
4. ‚úÖ Multiple metrics t·ªët h∆°n single metric

**Business:**
1. ‚úÖ ML kh√¥ng ph·∫£i silver bullet, c·∫ßn k·∫øt h·ª£p domain knowledge
2. ‚úÖ ƒê∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£ > Ph·ª©c t·∫°p nh∆∞ng kh√≥ maintain
3. ‚úÖ Monitor li√™n t·ª•c ƒë·ªÉ detect data drift
4. ‚úÖ Communicate k·∫øt qu·∫£ v·ªõi stakeholders

### 7.4. Impact Assessment

**Quantitative:**
- ‚¨áÔ∏è 44% th·ªùi gian ƒë√≥n kh√°ch
- ‚¨áÔ∏è 67% qu√£ng ƒë∆∞·ªùng
- ‚¨áÔ∏è 60% chi ph√≠ nhi√™n li·ªáu
- ‚¨áÔ∏è 25% s·ªë xe c·∫ßn

**Qualitative:**
- ‚¨ÜÔ∏è Tr·∫£i nghi·ªám kh√°ch h√†ng
- ‚¨ÜÔ∏è Hi·ªáu qu·∫£ v·∫≠n h√†nh
- ‚¨ÜÔ∏è L·ª£i nhu·∫≠n c√¥ng ty
- ‚¨ÜÔ∏è M√¥i tr∆∞·ªùng (gi·∫£m ph√°t th·∫£i)

### 7.5. K·∫øt lu·∫≠n cu·ªëi c√πng

> **Machine Learning clustering l√† gi·∫£i ph√°p hi·ªáu qu·∫£ cho b√†i to√°n ƒëi·ªÅu ph·ªëi xe, gi√∫p t·ªëi ∆∞u h√≥a l·ªô tr√¨nh v√† c·∫£i thi·ªán tr·∫£i nghi·ªám kh√°ch h√†ng. Model K-Means v·ªõi k=5 ƒë·∫°t k·∫øt qu·∫£ t·ªët (Silhouette 0.45), generalize t·ªët tr√™n test set, v√† s·∫µn s√†ng tri·ªÉn khai production.**

**Ti·∫øp theo:**
1. Deploy model l√™n production
2. Integrate v·ªõi dispatch system
3. Monitor performance
4. Continuously improve

---

## PH·ª§ L·ª§C

### A. C√¥ng th·ª©c to√°n h·ªçc chi ti·∫øt

**K-Means Objective:**

$$\text{argmin}_{C} \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

**Silhouette Coefficient:**

$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$

Trong ƒë√≥:
- $a(i)$: Avg distance to points in same cluster
- $b(i)$: Avg distance to points in nearest cluster

**Davies-Bouldin Index:**

$$DB = \frac{1}{k}\sum_{i=1}^{k}\max_{i \neq j}\left(\frac{\sigma_i + \sigma_j}{d(c_i, c_j)}\right)$$

**Calinski-Harabasz:**

$$s = \frac{\text{tr}(B_k)}{\text{tr}(W_k)} \times \frac{n-k}{k-1}$$

### B. Code Examples

**Full Pipeline:**

```python
# 1. Load & Clean
df = pd.read_csv('bookings.csv')
df_clean = clean_data(df)

# 2. Feature Engineering
features = extract_features(df_clean)
X = features[['lat', 'lng', 'time']].values

# 3. Split
X_train, X_test = train_test_split(X, test_size=0.2)

# 4. Scale & Weight
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_weighted = apply_weights(X_scaled)

# 5. Find k
optimal_k = find_optimal_k(X_weighted)

# 6. Train
model = KMeans(n_clusters=optimal_k)
model.fit(X_weighted)

# 7. Evaluate
metrics = evaluate_model(model, X_test)

# 8. Save
save_model(model, scaler, config)
```

### C. T√†i li·ªáu tham kh·∫£o

1. **Scikit-learn Documentation**: https://scikit-learn.org/stable/modules/clustering.html
2. **K-Means Tutorial**: https://stanford.edu/~cpiech/cs221/handouts/kmeans.html
3. **Cluster Validation**: Rousseeuw, P. J. (1987). Silhouettes: A graphical aid
4. **Machine Learning Yearning**: Andrew Ng

### D. Glossary (Thu·∫≠t ng·ªØ)

- **Clustering**: Ph√¢n c·ª•m
- **Centroid**: T√¢m c·ª•m
- **Inertia**: T·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch trong c·ª•m
- **Silhouette**: Ch·ªâ s·ªë ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng t·ª± trong/ngo√†i c·ª•m
- **Overfitting**: H·ªçc qu√° kh·ªõp
- **Generalization**: T·ªïng qu√°t h√≥a
- **Feature Engineering**: K·ªπ thu·∫≠t t·∫°o ƒë·∫∑c tr∆∞ng
- **Standardization**: Chu·∫©n h√≥a

---

**THE END**

*T√†i li·ªáu n√†y ƒë∆∞·ª£c t·∫°o ƒë·ªÉ h·ªó tr·ª£ b√°o c√°o d·ª± √°n ph√¢n c·ª•m kh√°ch h√†ng ƒë·∫∑t xe. M·ªçi th·∫Øc m·∫Øc xin li√™n h·ªá.*
